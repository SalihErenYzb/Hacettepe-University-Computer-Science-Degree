{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Student Name:Salih Eren Yüzbaşıoğlu Studen ID:2220356040\n",
    "\n",
    "KNN Learning Algorithm \n",
    "\n",
    "In this assignment we are given a data set containing 60000 rows of information about 60 questions that were asked to \n",
    "60000 people and their answer scaling from -3 to 3 and at the end their character found by doctors. Our duty is to make an algorithm to guess people's charecters by looking at the questions that were asked to them by using K nearest algoritm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "def read_input(data_to_read):\n",
    "    data=pd.read_csv(data_to_read,encoding='latin1')\n",
    "    data = data.sample(frac = 1)\n",
    "    data=data.iloc[:,1:]\n",
    "\n",
    "    data[\"Personality\"]=(data[\"Personality\"]==\"ESTJ\")*0 | (data[\"Personality\"]==\"ENTJ\")*1 | (data[\"Personality\"]==\"ESFJ\")*2 | (data[\"Personality\"]==\"ENFJ\")*3 |(data[\"Personality\"]==\"ISTJ\")*4 | (data[\"Personality\"]==\"ISFJ\")*5 | (data[\"Personality\"]==\"INTJ\")*6 | (data[\"Personality\"]==\"INFJ\")*7 | (data[\"Personality\"]==\"ESTP\")*8 | (data[\"Personality\"]==\"ESFP\")*9 | (data[\"Personality\"]==\"ENTP\")*10 | (data[\"Personality\"]==\"ENFP\")*11 | (data[\"Personality\"]==\"ISTP\")*12 | (data[\"Personality\"]==\"ISFP\")*13 | (data[\"Personality\"]==\"INTP\")*14 |(data[\"Personality\"]==\"INFP\")*15 \n",
    "    data=data.astype(int)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the library that we were supposed to use. Read_input basically reads the given excell file and turns it into pandas DataFrame then it shuffles the data and deletes the bir column since it is response id useless for our project. Then gives integer values for ever personality and turns the whole data to integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    normalized_data=(data+3)/6\n",
    "    normalized_data[\"Personality\"]=data[\"Personality\"]\n",
    "    return normalized_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not in read_input because we will do both normalized and not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Knn(k,whole_info_el,k_amount,data,len_Data):\n",
    "    Guess_amount_list=np.zeros([16,16], dtype = int)\n",
    "    Wrong_guesses=[]\n",
    "    test_len=len_Data//k_amount\n",
    "    \n",
    "    all_indexes=[(test_len*constant) for constant in range(1,k_amount+1)]\n",
    "    percen_constant=(100/test_len)/k_amount\n",
    "\n",
    "    percentage=float(0)\n",
    "    for cross in range(k_amount):\n",
    "\n",
    "        learn1=data.iloc[:all_indexes[cross]-test_len]\n",
    "        learn2=+data.iloc[all_indexes[cross]:]\n",
    "        combine=[learn1,learn2]\n",
    "        learn=np.array(pd.concat(combine))\n",
    "\n",
    "        test=np.array(data.iloc[all_indexes[cross]-test_len:all_indexes[cross]])       \n",
    "        \n",
    "        for line in range(test_len):\n",
    "\n",
    "            percentage+=percen_constant\n",
    "            print(f\"completed: {int(percentage)}%\",end='\\r')\n",
    "\n",
    "\n",
    "            difference=learn[:,:-1]-test[line,:-1]\n",
    "\n",
    "            distance=np.einsum('ij,ij->i',difference,difference)\n",
    "\n",
    "            smallest_k=np.argpartition(distance, k)[:k]\n",
    "            most_present=np.argmax(np.bincount(learn[ smallest_k,-1].astype(int)))\n",
    "\n",
    "\n",
    "            if int(test[line,-1])!=most_present:\n",
    "                Wrong_guesses.append([int(test[line,-1]),most_present,smallest_k])\n",
    "\n",
    "            Guess_amount_list[int(test[line,-1]),most_present]+=1\n",
    "    whole_info_el[\"Wrong_guesses\"]=Wrong_guesses\n",
    "    whole_info_el[\"Confusion_list\"]=Guess_amount_list\n",
    "\n",
    "    return Guess_amount_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the man in KNN function that takes data , a fairly empty dictionary to fill the results with , lenght of the data , k_amount corresponding the k in k-cross validation and k corresponding the k in KNN. Function starts by creating confusion matrix named Guess_amount_list this matrix will later be added to dictionary after we fill it completely, function also creates wrong_guesses list that will contain every wrong guess which also will be added to dictionary and will be used to determine why the guesses were wrong and investigation. After that we create a list containg the indexes that we wants cut the data in order for this function to be flexible with other datas I made it so it checks data lenght and divides it by k_amount . There is also percen_constant which is just a constant that we add to percentage every loop so and print percentage to know how much works is left. After that we have a for that turns for range of k_amount so in this example it turns 5 times for 5 cross validation, in for we see the defining of the learn and test arrays after that there is for loop for every line in test in for loop we print the percentage and make sure it is '\\r' so we see it live after that we find the difference between learn array and test array's line then to find distance we find np.einsum('ij,ij->i',difference,difference) which basically takes the square of every line in difference array and sums every line donated as j in a very efficient way. Then we use argpartition to find the smallest k elements which is also very efficient because it only does not spend time sorting the whole array but just finds smallest k amount then by combining bincount and argmax we first find how many times a value has been used with bincount then using argmax we find the biggest of them after that we check our guess and the real value if it is different we add it to wrong_guesses and also we upgrade confusion matrix with last value and after for loop we add the matrix and wrongs to dictionary and return confusion matrix so we can evaluate the precision ,recall and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_save_and_print_info(Guess_amount_list,whole_info_el,seconds,len_Data):\n",
    "\n",
    "    accuracy=round((Guess_amount_list.diagonal().sum()/len_Data)*100,4)\n",
    "    precision=round(((Guess_amount_list.diagonal()/Guess_amount_list.sum(-1)).sum()/16)*100,4)\n",
    "    recall=round(((Guess_amount_list.diagonal()/Guess_amount_list.sum(0)).sum()/16)*100,4)\n",
    "    whole_info_el[\"Precision\"]=precision\n",
    "    whole_info_el[\"Accuracy\"]=accuracy\n",
    "    whole_info_el[\"Recall\"]=recall\n",
    "    print(f\"Accuracy: {accuracy}%\")\n",
    "    print(f\"Recall: {recall}%\")\n",
    "    print(f\"Precision: {precision}%\")\n",
    "    print(f\"Time past: {round(time.time()-seconds,2)} seconds\")\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a dictionary too and using the confusion matrix named as Guess_amount_list to find precision,recall,accuracy. For recall and accuracy it uses macro average to add them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_algorithm(data_to_read,k_amount,*all_ks):\n",
    "    totaltime=time.time()\n",
    "    \n",
    "    whole_info={}\n",
    "\n",
    "    data=read_input(data_to_read)\n",
    "    \n",
    "\n",
    "\n",
    "    for normalization in range(2):\n",
    "        for k in all_ks:\n",
    "                if normalization==1:\n",
    "                        data=normalize(data)\n",
    "\n",
    "                        \n",
    "                        \n",
    "                print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "                print(f\"k constant: {k}\\t Normalization: \"+ ((\"Inactive \"*(normalization==0) or \"Active \")))\n",
    "                seconds = time.time()\n",
    "\n",
    "\n",
    "\n",
    "                whole_index=f\"{k}, N:\"+ (\"Inactive\"*(normalization==0) or \"Active\")\n",
    "\n",
    "                whole_info[whole_index]={\"Wrong_guesses\":[],\"Confusion_list\":None,\"Precision\":0,\"Accuracy\":0,\"Recall\":0}\n",
    "                whole_element=whole_info[whole_index]\n",
    "                len_Data=len(data)\n",
    "                after_knn=Knn(k,whole_element,k_amount,data,len_Data)\n",
    "                calculate_save_and_print_info(after_knn,whole_element,seconds,len_Data)\n",
    "\n",
    "    print(f\"Total time: {round(time.time()-totaltime)}\")\n",
    "\n",
    "    return whole_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function's whole purpose is to return a nested dictionary that contains every information for every k and activation it does this by calling other functions. First it finds time to figure out total time then reads data named data_to_read parameter given by the user then there are 2 for loops first one for normalization and second for every k that was given by the user. It prints necesseary information and calls the functions above. Most import thing here is it creates a dictioonary and it creates a another dictionary inside this one named f\"{k}, N:\"+ (\"Inactive\"*(normalization==0) or \"Active\") and while calling functions it does not give the outer dictionary but it gives the one the inside so every function that adds something to this dictionary add it to the one named f\"{k}, N:\"+ (\"Inactive\"*(normalization==0) or \"Active\") so none of them overlaps.And after everything it returns this huge dictionary that contains every informaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "k constant: 1\t Normalization: Inactive \n",
      "Accuracy: 97.785%\n",
      "Recall: 97.7914%\n",
      "Precision: 97.7911%\n",
      "Time past: 328.88 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "k constant: 3\t Normalization: Inactive \n",
      "Accuracy: 98.8616%\n",
      "Recall: 98.8684%\n",
      "Precision: 98.8685%\n",
      "Time past: 342.06 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "k constant: 5\t Normalization: Inactive \n",
      "Accuracy: 98.9066%\n",
      "Recall: 98.9133%\n",
      "Precision: 98.9135%\n",
      "Time past: 338.2 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "k constant: 7\t Normalization: Inactive \n",
      "Accuracy: 98.9333%\n",
      "Recall: 98.94%\n",
      "Precision: 98.9402%\n",
      "Time past: 340.4 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "k constant: 9\t Normalization: Inactive \n",
      "Accuracy: 98.93%\n",
      "Recall: 98.9366%\n",
      "Precision: 98.9368%\n",
      "Time past: 351.31 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "k constant: 1\t Normalization: Active \n",
      "Accuracy: 97.79%\n",
      "Recall: 97.7963%\n",
      "Precision: 97.7961%\n",
      "Time past: 603.4 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "k constant: 3\t Normalization: Active \n",
      "Accuracy: 98.8683%\n",
      "Recall: 98.8751%\n",
      "Precision: 98.8752%\n",
      "Time past: 592.46 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "k constant: 5\t Normalization: Active \n",
      "Accuracy: 98.9233%\n",
      "Recall: 98.93%\n",
      "Precision: 98.9302%\n",
      "Time past: 619.33 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "k constant: 7\t Normalization: Active \n",
      "Accuracy: 98.93%\n",
      "Recall: 98.9366%\n",
      "Precision: 98.9368%\n",
      "Time past: 628.93 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "k constant: 9\t Normalization: Active \n",
      "Accuracy: 98.9316%\n",
      "Recall: 98.9383%\n",
      "Precision: 98.9385%\n",
      "Time past: 623.49 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Total time: 4769\n"
     ]
    }
   ],
   "source": [
    "whole_info=full_algorithm(\"16P.csv\",5,1,3,5,7,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the run code as you can see k in cross validation is 5 and we call this function for 1,3,5,7,9 values in KNN\n",
    "here is the output of this code:\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "k constant: 1\t Normalization: Inactive \n",
    "Accuracy: 97.785%\n",
    "Recall: 97.7914%\n",
    "Precision: 97.7911%\n",
    "Time past: 328.88 seconds\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "k constant: 3\t Normalization: Inactive \n",
    "Accuracy: 98.8616%\n",
    "Recall: 98.8684%\n",
    "Precision: 98.8685%\n",
    "Time past: 342.06 seconds\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "k constant: 5\t Normalization: Inactive \n",
    "Accuracy: 98.9066%\n",
    "Recall: 98.9133%\n",
    "Precision: 98.9135%\n",
    "Time past: 338.2 seconds\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "k constant: 7\t Normalization: Inactive \n",
    "Accuracy: 98.9333%\n",
    "Recall: 98.94%\n",
    "Precision: 98.9402%\n",
    "Time past: 340.4 seconds\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "k constant: 9\t Normalization: Inactive \n",
    "Accuracy: 98.93%\n",
    "Recall: 98.9366%\n",
    "Precision: 98.9368%\n",
    "Time past: 351.31 seconds\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "k constant: 1\t Normalization: Active \n",
    "Accuracy: 97.79%\n",
    "Recall: 97.7963%\n",
    "Precision: 97.7961%\n",
    "Time past: 603.4 seconds\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "k constant: 3\t Normalization: Active \n",
    "Accuracy: 98.8683%\n",
    "Recall: 98.8751%\n",
    "Precision: 98.8752%\n",
    "Time past: 592.46 seconds\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "k constant: 5\t Normalization: Active \n",
    "Accuracy: 98.9233%\n",
    "Recall: 98.93%\n",
    "Precision: 98.9302%\n",
    "Time past: 619.33 seconds\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "k constant: 7\t Normalization: Active \n",
    "Accuracy: 98.93%\n",
    "Recall: 98.9366%\n",
    "Precision: 98.9368%\n",
    "Time past: 628.93 seconds\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "k constant: 9\t Normalization: Active \n",
    "Accuracy: 98.9316%\n",
    "Recall: 98.9383%\n",
    "Precision: 98.9385%\n",
    "Time past: 623.49 seconds\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'whole_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13416\\2949261298.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#ERROR FINDINGS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#first let's check some wrong guesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwhole_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"5, N:Inactive\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Confusion_list\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#reason I turned it into list is because it was not able fit into the line when it was array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m###output:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'whole_info' is not defined"
     ]
    }
   ],
   "source": [
    "#ERROR FINDINGS\n",
    "#first let's check some wrong guesses\n",
    "for k in whole_info[\"5, N:Inactive\"][\"Confusion_list\"]:\n",
    "    print(list(k))#reason I turned it into list is because it was not able fit into the line when it was array\n",
    "###output:\n",
    "#[3720, 3, 4, 3, 3, 2, 1, 2, 3, 2, 1, 6, 2, 2, 1, 4]\n",
    "#[1, 3705, 4, 3, 1, 2, 1, 2, 2, 3, 3, 1, 0, 3, 4, 2]\n",
    "#[1, 2, 3707, 3, 3, 1, 6, 2, 1, 2, 2, 5, 1, 2, 5, 3]\n",
    "#[2, 1, 4, 3707, 0, 1, 2, 4, 2, 1, 3, 0, 3, 5, 4, 3]\n",
    "#[1, 4, 2, 0, 3712, 5, 3, 4, 0, 2, 1, 5, 3, 3, 6, 5]\n",
    "#[5, 4, 2, 3, 1, 3701, 4, 1, 3, 2, 3, 1, 2, 1, 2, 4]\n",
    "#[1, 2, 1, 1, 1, 3, 3708, 3, 5, 1, 2, 1, 4, 3, 1, 6]\n",
    "#[3, 3, 2, 2, 0, 2, 2, 3721, 4, 5, 3, 1, 0, 0, 10, 3]\n",
    "#[1, 4, 5, 1, 1, 3, 4, 1, 3714, 3, 1, 2, 2, 2, 1, 4]\n",
    "#[4, 6, 3, 6, 2, 6, 5, 3, 4, 3713, 2, 1, 2, 3, 5, 4]\n",
    "#[4, 2, 2, 2, 2, 2, 0, 4, 2, 3, 3718, 3, 3, 3, 5, 5]\n",
    "#[4, 0, 4, 6, 3, 7, 5, 2, 1, 2, 2, 3713, 3, 4, 1, 3]\n",
    "#[1, 4, 5, 5, 7, 4, 4, 1, 2, 3, 2, 1, 3705, 5, 2, 3]\n",
    "#[1, 0, 5, 2, 3, 3, 4, 1, 2, 1, 1, 2, 2, 3708, 5, 5]\n",
    "#[2, 3, 3, 5, 3, 4, 4, 2, 3, 3, 4, 3, 3, 2, 3694, 3]\n",
    "#[1, 5, 3, 4, 4, 1, 1, 1, 1, 2, 3, 1, 3, 4, 3, 3697]\n",
    "# As you can there is no \tflocculation at all almost every number other than true ones are  smaller than 5 so I think \n",
    "# the reason for 1 percent error is because data might be flawed little and also it might not be big enough maybe with\n",
    "# a data set with 1 million rows would give a accuracy rate more than 99.9 percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error findings continues\n",
    "If you look at the original output you would see that best Neighbor Number is 7 I guess it is because anything more than 7 causes high bias and lower variance so it turns out best is 5\n",
    "One of the interesting thing was normalization effect was about 0.01 percent and it generally increased the accuracy so it was not a significant part f the project\n",
    "I can't think of a reason my algorithm would have any effect on accuracy since what it does is completely mathmatical and there is no room for any chance but my algorithm is quite flexible since you can the data or the k values or cross validation values by just calling the function with different values\n",
    "One of the other interesting things about this result is accuracy,recall and precision all have very similar values but this can be explained by looking at the confusion list again as you can see there is no flocculation so similarity between this values "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
